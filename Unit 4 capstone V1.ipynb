{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About The Data\n",
    "For this project, I have used the corpus of State of the Union addresses with the goal of classifing the texts using a combination of supervised and unsupervised learning techniques. The corpus comes from nltk and contains 65 State of the Union addresses from ten different US presidents. For my project, I have considered only 4 presidents.\n",
    "\n",
    "# Sections:\n",
    "\n",
    "•\tText Processing\n",
    "\n",
    "•\tsupervised Feature Generation\n",
    "\n",
    "•\tUnsupervised Feature Generation\n",
    "\n",
    "•\tSupervised Learning Models\n",
    "\n",
    "•\tComparing Supervised and Unsupervised Learning for NLP Applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import state_union, stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict, GridSearchCV, train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import neural_network\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth,  KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import confusion_matrix, auc, precision_recall_curve\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "The first step is to get each speech for the corpus. \n",
    "I will read the files president wise and and store them in a list and then create a loop to break each file into sentence level documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Truman = []\n",
    "\n",
    "Johnson = []\n",
    "Clinton = []\n",
    "GWBush = []\n",
    "for i in state_union.fileids():\n",
    "    if 'Truman' in i:\n",
    "        Truman.append(state_union.raw(i))\n",
    "\n",
    "    if 'Johnson' in i:\n",
    "        Johnson.append(state_union.raw(i))\n",
    "    if 'Clinton' in i:\n",
    "        Clinton.append(state_union.raw(i))\n",
    "    if 'GWBush' in i:\n",
    "        GWBush.append(state_union.raw(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Truman_speech = ' '.join(Truman)\n",
    "\n",
    "Johnson_speech = ' '.join(Johnson)\n",
    "Clinton_speech = ' '.join(Clinton)\n",
    "GWBush_speech = ' '.join(GWBush)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    text = ' '.join(text.split())\n",
    "     # Get rid of words in square brackets.\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_first_sentence(text):\n",
    "    text2 = ''\n",
    "    text2 = text.replace(text[0:text.find('\\n')],'')\n",
    "    return(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Clinton_speech = remove_first_sentence(Clinton_speech)\n",
    "Clinton_speech_cleaned = ''.join(text_cleaner(Clinton_speech))\n",
    "\n",
    "Truman = remove_first_sentence(Truman_speech)\n",
    "Truman_speech_cleaned = ''.join(text_cleaner(Truman_speech))\n",
    "\n",
    "Johnson = remove_first_sentence(Johnson_speech)\n",
    "Johnson_speech_cleaned = ''.join(text_cleaner(Johnson_speech))\n",
    "\n",
    "GWBush = remove_first_sentence(GWBush_speech)\n",
    "GWBush_speech_cleaned = ''.join(text_cleaner(GWBush_speech))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the few lines of one file before cleaning and after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n \\nFebruary 17, 1993 \\n\\nMr. President, Mr. Speaker, Members of the House and the Senate, distinguished Americans here as visitors in this Chamber, as am I. It is nice to have a fresh excuse for giving a long speech. [Laughter]\\nWhen Presidents speak to Congress and the Nation from this podium, typically they comment on the full range and challenges and opportunities that face the United States. But this is not an ordinary time, and for all the many tasks that require our attention, I believe tonig'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Clinton_speech[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'February 17, 1993 Mr. President, Mr. Speaker, Members of the House and the Senate, distinguished Americans here as visitors in this Chamber, as am I. It is nice to have a fresh excuse for giving a long speech.  When Presidents speak to Congress and the Nation from this podium, typically they comment on the full range and challenges and opportunities that face the United States. But this is not an ordinary time, and for all the many tasks that require our attention, I believe tonight one calls on'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Clinton_speech_cleaned[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning looks good. Now that we have our documents cleaned up , let's tokenize the sentences into words and get the lemmas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse using SpaCy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Clinton_doc = nlp(Clinton_speech_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Truman_doc = nlp(Truman_speech_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Johnson_doc = nlp(Johnson_speech_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GWBush_doc = nlp(GWBush_speech_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(February, 17, ,, 1993, Mr., President, ,, Mr....</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(It, is, nice, to, have, a, fresh, excuse, for...</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(When, Presidents, speak, to, Congress, and, t...</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(But, this, is, not, an, ordinary, time, ,, an...</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(And, that, is, our, economy, .)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (February, 17, ,, 1993, Mr., President, ,, Mr....  Clinton\n",
       "1  (It, is, nice, to, have, a, fresh, excuse, for...  Clinton\n",
       "2  (When, Presidents, speak, to, Congress, and, t...  Clinton\n",
       "3  (But, this, is, not, an, ordinary, time, ,, an...  Clinton\n",
       "4                   (And, that, is, our, economy, .)  Clinton"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group into sentences\n",
    "Clinton_sents = [ [sent,'Clinton']  for sent in Clinton_doc.sents]\n",
    "Truman_sents =  [ [sent, 'Truman']  for sent in Truman_doc.sents]\n",
    "Johnson_sents = [ [sent, 'Johnson'] for sent in Johnson_doc.sents]\n",
    "GWBush_sents =  [ [sent,'GWBush']   for sent in GWBush_doc.sents]\n",
    "\n",
    "\n",
    "sentences_df = pd.DataFrame(Clinton_sents + Truman_sents + \n",
    "                            Johnson_sents + GWBush_sents)\n",
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9795, 2)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_df.columns= ['sent','president']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Clinton    32.363451\n",
       "Truman     26.778969\n",
       "GWBush     22.399183\n",
       "Johnson    18.458397\n",
       "Name: president, dtype: float64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.president.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Feature Generation \n",
    "Bag of Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create bag of words function for each text\n",
    "def bag_of_words(text, most_common_count, person):\n",
    "    \n",
    "    # filter out punctuation and stop words\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop\n",
    "               ]\n",
    "    \n",
    "    \n",
    "    # Return most common words\n",
    "    return [item[0] for item in Counter(allwords).most_common(most_common_count)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, let's try taking only the 500 most common words from each president"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get bags \n",
    "Clinton_words = bag_of_words(Clinton_doc, 500, 'Clinton')\n",
    "Truman_words = bag_of_words(Truman_doc, 500, 'Truman')\n",
    "\n",
    "Johnson_words = bag_of_words(Johnson_doc, 500, 'Johnson')\n",
    "GWBush_words = bag_of_words(GWBush_doc, 500, 'GWBush')\n",
    "\n",
    "\n",
    "# Combine bags to create common set of unique words\n",
    "common_words = set(Clinton_words + Truman_words +\n",
    "                   Johnson_words + GWBush_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872\n"
     ]
    }
   ],
   "source": [
    "print(len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create bag of words data frame using combined common words and sentences\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Build data frame\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences['sent']\n",
    "    df['text_source'] = sentences['president']\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentences in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentences\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>courage</th>\n",
       "      <th>across</th>\n",
       "      <th>welcome</th>\n",
       "      <th>secure</th>\n",
       "      <th>10</th>\n",
       "      <th>strong</th>\n",
       "      <th>care</th>\n",
       "      <th>marriage</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>task</th>\n",
       "      <th>person</th>\n",
       "      <th>current</th>\n",
       "      <th>water</th>\n",
       "      <th>deserve</th>\n",
       "      <th>gather</th>\n",
       "      <th>destruction</th>\n",
       "      <th>condition</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(February, 17, ,, 1993, Mr., President, ,, Mr....</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(It, is, nice, to, have, a, fresh, excuse, for...</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(When, Presidents, speak, to, Congress, and, t...</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(But, this, is, not, an, ordinary, time, ,, an...</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(And, that, is, our, economy, .)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 874 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  best courage across welcome secure 10 strong care marriage  2     ...      \\\n",
       "0    0       0      0       0      0  0      0    0        0  0     ...       \n",
       "1    0       0      0       0      0  0      0    0        0  0     ...       \n",
       "2    0       0      0       0      0  0      0    0        0  0     ...       \n",
       "3    0       0      0       0      0  0      0    0        0  0     ...       \n",
       "4    0       0      0       0      0  0      0    0        0  0     ...       \n",
       "\n",
       "  task person current water deserve gather destruction condition  \\\n",
       "0    0      0       0     0       0      0           0         0   \n",
       "1    0      0       0     0       0      0           0         0   \n",
       "2    0      0       0     0       0      0           0         0   \n",
       "3    1      0       0     0       0      0           0         0   \n",
       "4    0      0       0     0       0      0           0         0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (February, 17, ,, 1993, Mr., President, ,, Mr....     Clinton  \n",
       "1  (It, is, nice, to, have, a, fresh, excuse, for...     Clinton  \n",
       "2  (When, Presidents, speak, to, Congress, and, t...     Clinton  \n",
       "3  (But, this, is, not, an, ordinary, time, ,, an...     Clinton  \n",
       "4                   (And, that, is, our, economy, .)     Clinton  \n",
       "\n",
       "[5 rows x 874 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create bow features \n",
    "speech_df = bow_features(sentences_df, common_words)\n",
    "speech_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9795, 874)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling on BOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_split():\n",
    "    X_bow = speech_df.drop(['text_sentence', 'text_source'], 1)\n",
    "    Y_bow = speech_df['text_source']\n",
    "    X_train_bow, X_test_bow,y_train_bow, y_test_bow= train_test_split(X_bow,Y_bow, test_size=0.2, random_state=0, stratify = Y_bow)\n",
    "    print('Training Data size\\n',X_train_bow.shape)\n",
    "    print('Test data size\\n',X_test_bow.shape)\n",
    "    return(X_train_bow, X_test_bow,y_train_bow, y_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def models_default_param(X_train, y_train,X_test,y_test):\n",
    "    \n",
    "    print('Logistic Regression')\n",
    "    lr = LogisticRegression()\n",
    "    lr_bow = lr.fit(X_train, y_train)\n",
    "    lr_train_score =  cross_val_score(lr_bow, X_train, y_train, cv=5)\n",
    "    lr_test_score = cross_val_score(lr_bow, X_test,y_test, cv=5)\n",
    "    print('Score : \\n ',lr_train_score)\n",
    "    print('Training Data Avg Score:', np.mean(lr_train_score))\n",
    "    print('Test Data Avg Score:', np.mean(lr_test_score))\n",
    "    print()\n",
    "    \n",
    "    print('Random Forest Classifier')\n",
    "    rfc = ensemble.RandomForestClassifier()\n",
    "    rfc_bow = rfc.fit(X_train, y_train)\n",
    "    rfc_train_score =  cross_val_score(rfc_bow, X_train, y_train, cv=5)\n",
    "    rfc_test_score = cross_val_score(rfc_bow, X_test,y_test, cv=5)\n",
    "    print('Score : \\n ',rfc_train_score)\n",
    "    print('Training Data Avg Score:', np.mean(rfc_train_score))\n",
    "    print('Test Data Avg Score:', np.mean(rfc_test_score))\n",
    "    print()\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models with 500 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data size\n",
      " (7836, 872)\n",
      "Test data size\n",
      " (1959, 872)\n",
      "\n",
      "Logistic Regression\n",
      "Score : \n",
      "  [ 0.63989802  0.66560306  0.65475431  0.63369496  0.69667944]\n",
      "Training Data Avg Score: 0.658125958314\n",
      "Test Data Avg Score: 0.590076259119\n",
      "\n",
      "Random Forest Classifier\n",
      "Score : \n",
      "  [ 0.53983429  0.54562859  0.55009572  0.53605616  0.53001277]\n",
      "Training Data Avg Score: 0.540325506598\n",
      "Test Data Avg Score: 0.479812115195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test,y_train, y_test = bow_split()\n",
    "print()\n",
    "models_default_param(X_train, y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores are not bad. Let's try to improve the scores by taking more common words and then considering other aspects too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get bags \n",
    "\n",
    "Clinton_words = bag_of_words(Clinton_doc, 1500, 'Clinton')\n",
    "Truman_words = bag_of_words(Truman_doc, 1500, 'Truman')\n",
    "Johnson_words = bag_of_words(Johnson_doc, 1500, 'Johnson')\n",
    "GWBush_words = bag_of_words(GWBush_doc, 1500, 'GWBush')\n",
    "\n",
    "\n",
    "# Combine bags to create common set of unique words\n",
    "common_words = set(Clinton_words + Truman_words + \n",
    "                   Johnson_words + GWBush_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def bow_features(sentences, common_words):   \n",
    "   \n",
    "    \n",
    "    df = pd.DataFrame(columns= set(list(common_words) ))\n",
    "\n",
    "    df['text_sentence'] = sentences['sent']\n",
    "    df['text_source'] = sentences['president']\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    df['sent_length'] = 0\n",
    "    \n",
    "    df['prev_sent_length'] = 0\n",
    "    df['next_sent_length'] = 0\n",
    "    df['num_words_repeated_from_prior_sent'] = 0\n",
    "    \n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "          #Check to see if each phrase turns up in the sentence (store as binary var for the time being)       \n",
    "                \n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        #Also add # of repeated words from one sentence to the next\n",
    "        repeats = 0\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "            if i > 0: \n",
    "                if ((df.loc[i-1, word] > 0) & (df.loc[i, word] > 0)):\n",
    "                    repeats += 1\n",
    "            else: \n",
    "                repeats = 0\n",
    "        df['num_words_repeated_from_prior_sent'][i] = repeats        \n",
    "\n",
    "        sent_len = 0    \n",
    "        num_punct = 0 \n",
    "        \n",
    "        for token in sentence:\n",
    "        \n",
    "            if not token.is_punct:\n",
    "                sent_len += 1\n",
    "            else:\n",
    "                num_punct += 1\n",
    "        df.loc[i, 'sent_length'] = sent_len\n",
    "        df.loc[i, 'sent_punct_count'] = num_punct\n",
    "        \n",
    "        if i > 0:\n",
    "            df.loc[i, 'prev_sent_length'] = df.loc[i-1, 'sent_length']\n",
    "        else:\n",
    "            df.loc[i, 'prev_sent_length'] = 0\n",
    "                              \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "                      \n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "    #Back out of the loop through sentences and just shift the df by one to get the \"next sent len\" feature\n",
    "    df['next_sent_length'] = df['sent_length'].shift(-1)         \n",
    "                \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gyans\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n",
      "Processing row 5500\n",
      "Processing row 6000\n",
      "Processing row 6500\n",
      "Processing row 7000\n",
      "Processing row 7500\n",
      "Processing row 8000\n",
      "Processing row 8500\n",
      "Processing row 9000\n",
      "Processing row 9500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>welcome</th>\n",
       "      <th>define</th>\n",
       "      <th>pleased</th>\n",
       "      <th>settle</th>\n",
       "      <th>pentagon</th>\n",
       "      <th>iii</th>\n",
       "      <th>2</th>\n",
       "      <th>expenditure</th>\n",
       "      <th>flourishing</th>\n",
       "      <th>...</th>\n",
       "      <th>gather</th>\n",
       "      <th>catch</th>\n",
       "      <th>scarcity</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>prev_sent_length</th>\n",
       "      <th>next_sent_length</th>\n",
       "      <th>num_words_repeated_from_prior_sent</th>\n",
       "      <th>sent_punct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(February, 17, ,, 1993, Mr., President, ,, Mr....</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(It, is, nice, to, have, a, fresh, excuse, for...</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(When, Presidents, speak, to, Congress, and, t...</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(But, this, is, not, an, ordinary, time, ,, an...</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(And, that, is, our, economy, .)</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2793 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  best welcome define pleased settle pentagon iii  2 expenditure flourishing  \\\n",
       "0    0       0      0       0      0        0   0  0           0           0   \n",
       "1    0       0      0       0      0        0   0  0           0           0   \n",
       "2    0       0      0       0      0        0   0  0           0           0   \n",
       "3    0       0      0       0      0        0   0  0           0           0   \n",
       "4    0       0      0       0      0        0   0  0           0           0   \n",
       "\n",
       "        ...        gather catch scarcity  \\\n",
       "0       ...             0     0        0   \n",
       "1       ...             0     0        0   \n",
       "2       ...             0     0        0   \n",
       "3       ...             0     0        0   \n",
       "4       ...             0     0        0   \n",
       "\n",
       "                                       text_sentence text_source sent_length  \\\n",
       "0  (February, 17, ,, 1993, Mr., President, ,, Mr....     Clinton          25   \n",
       "1  (It, is, nice, to, have, a, fresh, excuse, for...     Clinton          14   \n",
       "2  (When, Presidents, speak, to, Congress, and, t...     Clinton          27   \n",
       "3  (But, this, is, not, an, ordinary, time, ,, an...     Clinton          31   \n",
       "4                   (And, that, is, our, economy, .)     Clinton           5   \n",
       "\n",
       "  prev_sent_length next_sent_length num_words_repeated_from_prior_sent  \\\n",
       "0                0             14.0                                  0   \n",
       "1               25             27.0                                  1   \n",
       "2               14             31.0                                  2   \n",
       "3               27              5.0                                 12   \n",
       "4               31             17.0                                  4   \n",
       "\n",
       "  sent_punct_count  \n",
       "0              5.0  \n",
       "1              1.0  \n",
       "2              2.0  \n",
       "3              5.0  \n",
       "4              1.0  \n",
       "\n",
       "[5 rows x 2793 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create bow features \n",
    "speech_df = bow_features(sentences_df, common_words)\n",
    "speech_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entityty_types(df):\n",
    "    \n",
    "    person_ent_type = []\n",
    "    qty_ent_type = []\n",
    "    ordinal_ent_type = []\n",
    "    time_ent_type = []\n",
    "    org_ent_type = []\n",
    "    lang_ent_type = []\n",
    "    date_ent_type = []\n",
    "    card_ent_type = []\n",
    "    gpe_ent_type = []\n",
    "    fac_ent_type = []\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        person_count = 0\n",
    "        qty_count= 0\n",
    "        ordinal_count = 0\n",
    "        time_count = 0\n",
    "        org_count = 0\n",
    "        lang_count = 0\n",
    "        date_count= 0\n",
    "        cardinal_count =0 \n",
    "        gpe_count= 0\n",
    "        fac_count = 0\n",
    "    \n",
    "        for token in sentence:\n",
    "            if token.ent_type_ == 'PERSON':\n",
    "                person_count += 1\n",
    "        \n",
    "            if token.ent_type_ == 'QUANTITY':\n",
    "                qty_count += 1\n",
    "            \n",
    "            if token.ent_type_ == 'ORDINAL':\n",
    "                ordinal_count += 1\n",
    "            \n",
    "            if token.ent_type_ == 'TIME':\n",
    "                time_count += 1\n",
    "            \n",
    "            if token.ent_type_ == 'ORG':\n",
    "                org_count += 1\n",
    "            \n",
    "            if token.ent_type_ == 'LANGUAGE':\n",
    "                lang_count += 1\n",
    "            if token.ent_type_ == 'DATE':\n",
    "                date_count += 1            \n",
    "        \n",
    "            if token.ent_type_ == 'CARDINAL':\n",
    "                cardinal_count += 1            \n",
    "            if token.ent_type_ == 'GPE':\n",
    "                gpe_count += 1            \n",
    "            if token.ent_type_ == 'FAC':\n",
    "                fac_count += 1            \n",
    "            \n",
    "        person_ent_type.append(person_count)\n",
    "        qty_ent_type.append(qty_count)\n",
    "        ordinal_ent_type.append(ordinal_count)\n",
    "        time_ent_type.append(time_count)\n",
    "        org_ent_type.append(org_count)\n",
    "        lang_ent_type.append(lang_count)\n",
    "        date_ent_type.append(date_count)\n",
    "        card_ent_type.append(cardinal_count)\n",
    "        gpe_ent_type.append(gpe_count)\n",
    "        fac_ent_type.append(fac_count)\n",
    "\n",
    "          \n",
    "    df['person_ent'] = person_ent_type\n",
    "    df['qty_ent'] = qty_ent_type\n",
    "    df['ordinal_ent'] = ordinal_ent_type\n",
    "    df['time_ent'] = time_ent_type\n",
    "    df['org_ent'] = org_ent_type\n",
    "    df['lang_ent'] = lang_ent_type\n",
    "    df['date_ent'] = date_ent_type\n",
    "    df['card_ent'] = card_ent_type\n",
    "    df['gpe_ent'] = gpe_ent_type\n",
    "    df['fac_ent'] = fac_ent_type\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def grammars(df):\n",
    "    adv_count_list = []\n",
    "    verb_count_list = []\n",
    "    noun_count_list = []\n",
    "    propnoun_count_list = []\n",
    "    punc_count_list = []\n",
    "    #-----------------\n",
    "    part_cnt_list= []\n",
    "    adj_cnt_list= []\n",
    "    adp_cnt_list= []\n",
    "    det_cnt_list= []\n",
    "\n",
    "    for sentence in df['text_sentence']:\n",
    "        \n",
    "        advs_cnt = 0\n",
    "        verb_cnt = 0\n",
    "        noun_cnt = 0\n",
    "        propnoun_cnt = 0\n",
    "        punc_cnt = 0\n",
    "    #-----------------\n",
    "        part_cnt= 0\n",
    "        adj_cnt= 0\n",
    "        adp_cnt= 0\n",
    "        det_cnt= 0\n",
    "        \n",
    "        for token in sentence:\n",
    "            if token.pos_ == 'ADV':\n",
    "                advs_cnt +=1\n",
    "            if token.pos_ == 'VERB':\n",
    "                verb_cnt +=1\n",
    "            if token.pos_ == 'NOUN':\n",
    "                noun_cnt +=1\n",
    "            if token.pos_ == 'PROPN':\n",
    "                propnoun_cnt +=1\n",
    "            if token.pos_ == 'PUNCT':\n",
    "                punc_cnt +=1\n",
    "    #---------------------------------------\n",
    "            if token.pos_ == 'PART':\n",
    "                part_cnt +=1\n",
    "            if token.pos_ == 'ADJ':\n",
    "                adj_cnt +=1\n",
    "            if token.pos_ == 'ADP':\n",
    "                adp_cnt +=1\n",
    "            if token.pos_ == 'DET':\n",
    "                det_cnt +=1\n",
    "        \n",
    "        adv_count_list.append(advs_cnt)\n",
    "        verb_count_list.append(verb_cnt)\n",
    "        noun_count_list.append(noun_cnt)\n",
    "        propnoun_count_list.append(propnoun_cnt)\n",
    "        punc_count_list.append(punc_cnt)\n",
    "        #----------------------------------\n",
    "        part_cnt_list.append(part_cnt)\n",
    "        adj_cnt_list.append(adj_cnt)\n",
    "        adp_cnt_list.append(adp_cnt)\n",
    "        det_cnt_list.append(det_cnt)\n",
    "    #---------------------------------------    \n",
    "        \n",
    "    df['adv_count'] = adv_count_list\n",
    "    df['verb_count'] = verb_count_list\n",
    "    df['noun_count'] = noun_count_list\n",
    "    df['pronoun_count'] = propnoun_count_list\n",
    "    df['punc_count'] = punc_count_list\n",
    "\n",
    "    #-----------------\n",
    "    df['part_cnt'] = part_cnt_list\n",
    "    df['adj_cnt'] = adj_cnt_list\n",
    "    df['adp_cnt'] = adp_cnt_list\n",
    "    df['det_cnt'] = det_cnt_list\n",
    "    return(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speech_df = entityty_types(speech_df)\n",
    "speech_df = grammars(speech_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>welcome</th>\n",
       "      <th>define</th>\n",
       "      <th>pleased</th>\n",
       "      <th>settle</th>\n",
       "      <th>pentagon</th>\n",
       "      <th>iii</th>\n",
       "      <th>2</th>\n",
       "      <th>expenditure</th>\n",
       "      <th>flourishing</th>\n",
       "      <th>...</th>\n",
       "      <th>fac_ent</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>pronoun_count</th>\n",
       "      <th>punc_count</th>\n",
       "      <th>part_cnt</th>\n",
       "      <th>adj_cnt</th>\n",
       "      <th>adp_cnt</th>\n",
       "      <th>det_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2812 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  best welcome define pleased settle pentagon iii  2 expenditure flourishing  \\\n",
       "0    0       0      0       0      0        0   0  0           0           0   \n",
       "1    0       0      0       0      0        0   0  0           0           0   \n",
       "2    0       0      0       0      0        0   0  0           0           0   \n",
       "3    0       0      0       0      0        0   0  0           0           0   \n",
       "4    0       0      0       0      0        0   0  0           0           0   \n",
       "\n",
       "    ...   fac_ent adv_count verb_count noun_count pronoun_count punc_count  \\\n",
       "0   ...         0         1          1          1            11          5   \n",
       "1   ...         0         0          3          2             0          1   \n",
       "2   ...         0         2          3          5             4          2   \n",
       "3   ...         0         1          7          4             0          5   \n",
       "4   ...         0         0          1          1             0          1   \n",
       "\n",
       "  part_cnt adj_cnt adp_cnt det_cnt  \n",
       "0        0       1       4       3  \n",
       "1        1       3       1       2  \n",
       "2        0       2       3       4  \n",
       "3        3       5       2       3  \n",
       "4        0       1       0       1  \n",
       "\n",
       "[5 rows x 2812 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speech_df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data size\n",
      " (7836, 2810)\n",
      "Test data size\n",
      " (1959, 2810)\n",
      "\n",
      "Logistic Regression\n",
      "Score : \n",
      "  [ 0.69279796  0.69432036  0.7019783   0.68793874  0.73563218]\n",
      "Training Data Avg Score: 0.702533508138\n",
      "Test Data Avg Score: 0.603906815577\n",
      "\n",
      "Random Forest Classifier\n",
      "Score : \n",
      "  [ 0.56214149  0.56987875  0.5807275   0.57753669  0.59131545]\n",
      "Training Data Avg Score: 0.576319978618\n",
      "Test Data Avg Score: 0.523217751552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test,y_train, y_test = bow_split()\n",
    "print()\n",
    "models_default_param(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding few more features, the logistic regression scores for train data got 5% improvement, and test data got 1% improvement. For Random forest classifier, the train scores and test score got 4% improvement.\n",
    "\n",
    "Let's try hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data size\n",
      " (7836, 2810)\n",
      "Test data size\n",
      " (1959, 2810)\n",
      "Score : \n",
      "  [ 0.67941364  0.69176771  0.69432036  0.68921506  0.72030651]\n",
      "Training Data Avg Score: 0.695004655933\n",
      "Test Data Avg Score: 0.598301021776\n"
     ]
    }
   ],
   "source": [
    "# Do we need multi_class='multinomial' ??scores have reduced when tried this !!!\n",
    "X_train, X_test,y_train, y_test = bow_split()\n",
    "lr = LogisticRegression(solver='newton-cg', multi_class='multinomial')\n",
    "lr_bow = lr.fit(X_train, y_train)\n",
    "lr_train_score =  cross_val_score(lr_bow, X_train, y_train, cv=5)\n",
    "lr_test_score = cross_val_score(lr_bow, X_test,y_test, cv=5)\n",
    "print('Score : \\n ',lr_train_score)\n",
    "print('Training Data Avg Score:', np.mean(lr_train_score))\n",
    "print('Test Data Avg Score:', np.mean(lr_test_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data size\n",
      " (7836, 2810)\n",
      "Test data size\n",
      " (1959, 2810)\n",
      "Score : \n",
      "  [ 0.6736775   0.68155712  0.69049138  0.67262285  0.72349936]\n",
      "Training Data Avg Score: 0.688369641909\n",
      "Test Data Avg Score: 0.605442660787\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "# Do we need multi_class='multinomial' ??scores have reduced when tried this !!!\n",
    "X_train, X_test,y_train, y_test = bow_split()\n",
    "lrcv = LogisticRegressionCV()\n",
    "# solver='newton-cg', multi_class='multinomial'\n",
    "lrcv_bow = lrcv.fit(X_train, y_train)\n",
    "lrcv_train_score =  cross_val_score(lrcv_bow, X_train, y_train, cv=5)\n",
    "lrcv_test_score = cross_val_score(lrcv_bow, X_test,y_test, cv=5)\n",
    "print('Score : \\n ',lrcv_train_score)\n",
    "print('Training Data Avg Score:', np.mean(lrcv_train_score))\n",
    "print('Test Data Avg Score:', np.mean(lrcv_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter  {'C': 1, 'fit_intercept': True, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "lr_params =[ {'C': [0.01, 0.1, 1, 10],'solver':['liblinear'],'penalty':['l1', 'l2'],'fit_intercept':[True]} ]\n",
    "lr = LogisticRegression()\n",
    "gr_logr = GridSearchCV(lr,param_grid = lr_params )\n",
    "gr_logr.fit(X_train, y_train)\n",
    "print('Best Parameter ', gr_logr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter  {'max_depth': None, 'max_features': 'log2', 'min_samples_split': 2, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "rfc_params  = {\n",
    "    'n_estimators':[100,500],\n",
    "    'max_features':['auto', 'sqrt', 'log2'],\n",
    "    'max_depth':[4, 6,7, None],\n",
    "    'min_samples_split':[2, 8]\n",
    "}\n",
    "rfc = ensemble.RandomForestClassifier(random_state=10)\n",
    "rfc_grid = GridSearchCV(rfc, param_grid=rfc_params)\n",
    "rfc_grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Best Parameter ', rfc_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def models_best_param(X_train, y_train,X_test,y_test):\n",
    "    \n",
    "    print('Logistic Regression')\n",
    "    lr = LogisticRegression(**gr_logr.best_params_)\n",
    "    lr_bow = lr.fit(X_train, y_train)\n",
    "    lr_train_score_gs =  cross_val_score(lr_bow, X_train, y_train, cv=5)\n",
    "    lr_test_score_gs = cross_val_score(lr_bow, X_test,y_test, cv=5)\n",
    "    print('Score : \\n ',lr_train_score_gs)\n",
    "    print('Training Data Avg Score:', np.mean(lr_train_score_gs))\n",
    "    print('Test Data Avg Score:', np.mean(lr_test_score_gs))\n",
    "    print()\n",
    "    \n",
    "    print('Random Forest Classifier')\n",
    "    rfc = ensemble.RandomForestClassifier(**rfc_grid.best_params_)\n",
    "    rfc_bow = rfc.fit(X_train, y_train)\n",
    "    rfc_train_score_gs =  cross_val_score(rfc_bow, X_train, y_train, cv=5)\n",
    "    rfc_test_score_gs = cross_val_score(rfc_bow, X_test,y_test, cv=5)\n",
    "    print('Score : \\n ',rfc_train_score_gs)\n",
    "    print('Training Data Avg Score:', np.mean(rfc_train_score_gs))\n",
    "    print('Test Data Avg Score:', np.mean(rfc_test_score_gs))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Score : \n",
      "  [ 0.69279796  0.69432036  0.7019783   0.68793874  0.73563218]\n",
      "Training Data Avg Score: 0.702533508138\n",
      "Test Data Avg Score: 0.603906815577\n",
      "\n",
      "Random Forest Classifier\n",
      "Score : \n",
      "  [ 0.66794136  0.64964901  0.66560306  0.65092534  0.65581098]\n",
      "Training Data Avg Score: 0.657985951277\n",
      "Test Data Avg Score: 0.604874854041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_best_param(X_train, y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying hyperparameter tuning we did not see any improvement in Logistic regression scores, but the random forest scores are better now with 8% improvement for both train and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Feature Generation\n",
    "\n",
    " In this section, we'll explore two techniques for generating unsupervised features:\n",
    "\n",
    "•\tTf-idf\n",
    "\n",
    "•\tLatent Semantic Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Truman = []\n",
    "Johnson = []\n",
    "Clinton = []\n",
    "GWBush = []\n",
    "for i in state_union.fileids():\n",
    "    if 'Truman' in i:\n",
    "        truman_sents = state_union.sents(i)\n",
    "        truman_sent = [ \" \".join(sent) for sent in truman_sents]\n",
    "        del truman_sent[0]\n",
    "        Truman.append( truman_sent   )\n",
    "\n",
    "    if 'Johnson' in i:\n",
    "        Johnson_sents = state_union.sents(i)\n",
    "        Johnson_sent = [ \" \".join(sent) for sent in Johnson_sents]\n",
    "        del Johnson_sent[0]\n",
    "        Johnson.append( Johnson_sent   )\n",
    "        \n",
    "    if 'Clinton' in i:\n",
    "        Clinton_sents = state_union.sents(i)\n",
    "        Clinton_sent =   [ \" \".join(sent) for sent in Clinton_sents]\n",
    "        del Clinton_sent[0]\n",
    "        Clinton.append(Clinton_sent    )\n",
    "        \n",
    "    if 'GWBush' in i:\n",
    "        GWBush_sents = state_union.sents(i)\n",
    "        GWBush_sent = [ \" \".join(sent) for sent in GWBush_sents] \n",
    "        del GWBush_sent [0]\n",
    "        GWBush.append(  GWBush_sent  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Truman_sents_list = []\n",
    "Johnson_sents_list = []\n",
    "Clinton_sents_list = []\n",
    "GWBush_sents_list = []\n",
    "for sublist in Truman:\n",
    "    for item in sublist:\n",
    "        Truman_sents_list.append(item)  \n",
    "\n",
    "for sublist in Johnson:\n",
    "    for item in sublist:\n",
    "        Johnson_sents_list.append(item)  \n",
    "\n",
    "\n",
    "for sublist in Clinton:\n",
    "    for item in sublist:\n",
    "        Clinton_sents_list.append(item)  \n",
    "\n",
    "for sublist in GWBush:\n",
    "    for item in sublist:\n",
    "        GWBush_sents_list.append(item)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_sentences_df = pd.DataFrame(columns= ['sent', 'president'])\n",
    "tfidf_sentences_df['sent'] = Truman_sents_list\n",
    "tfidf_sentences_df['president'] = 'Truman'\n",
    "\n",
    "Johnson_df = pd.DataFrame(columns= ['sent', 'president'])\n",
    "Johnson_df['sent'] = Johnson_sents_list\n",
    "Johnson_df['president'] = 'Johnson'\n",
    "\n",
    "Clinton_df = pd.DataFrame(columns= ['sent', 'president'])\n",
    "Clinton_df['sent'] = Clinton_sents_list\n",
    "Clinton_df['president'] = 'Clinton'\n",
    "\n",
    "GWBush_df = pd.DataFrame(columns= ['sent', 'president'])\n",
    "GWBush_df['sent'] = GWBush_sents_list\n",
    "GWBush_df['president'] = 'GWBush'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_sentences_df = pd.concat([tfidf_sentences_df, Johnson_df,\n",
    "                                Clinton_df, GWBush_df ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>president</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>April 16 , 1945</td>\n",
       "      <td>Truman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr . Speaker , Mr . President , Members of the...</td>\n",
       "      <td>Truman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Only yesterday , we laid to rest the mortal re...</td>\n",
       "      <td>Truman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>At a time like this , words are inadequate .</td>\n",
       "      <td>Truman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The most eloquent tribute would be a reverent ...</td>\n",
       "      <td>Truman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent president\n",
       "0                                    April 16 , 1945    Truman\n",
       "1  Mr . Speaker , Mr . President , Members of the...    Truman\n",
       "2  Only yesterday , we laid to rest the mortal re...    Truman\n",
       "3       At a time like this , words are inadequate .    Truman\n",
       "4  The most eloquent tribute would be a reverent ...    Truman"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_sentences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9613, 2)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_sentences_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Clinton    3114\n",
       "Truman     2575\n",
       "GWBush     2153\n",
       "Johnson    1771\n",
       "Name: president, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_sentences_df.president.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_sentences_df['sent'],tfidf_sentences_df['president'],\n",
    "                                    stratify=tfidf_sentences_df['president'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7690"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2961                              A lot is riding on it .\n",
       "2745             I hope you will support that , as well .\n",
       "23      Another picture would be full of blessings : a...\n",
       "751                                 There is more to do .\n",
       "258                                         ( Applause .)\n",
       "1614    But we have to move ahead with courage and hon...\n",
       "1460                               The need is pressing .\n",
       "181     And now I understand why , having dealt with t...\n",
       "383     We have seen it in the courage of passengers ,...\n",
       "348     We froze domestic spending and used honest bud...\n",
       "937     That ' s why we worked so hard to increase edu...\n",
       "2365    The imperialism of the czars has been replaced...\n",
       "2929    I think you ought to do it for two reasons : F...\n",
       "635     And they are going to have those privileges of...\n",
       "129     Later this year , we will offer a plan to end ...\n",
       "1204    Now it is time for us to look also to the chal...\n",
       "1181    Twenty - eight months have passed since Septem...\n",
       "598     The Congress has a clear responsibility to mee...\n",
       "49                 In short , this is no time for delay .\n",
       "1019    The interest charges on it amount to but a sma...\n",
       "465     He collects it from the scattered hopes of the...\n",
       "620     A very proper requirement , in my opinion , wo...\n",
       "169     All members of the public should have an equal...\n",
       "897     I don ' t want to destroy the good atmosphere ...\n",
       "1695    Our gross national product has grown more in t...\n",
       "1068    The job of the inspectors is to verify that Ir...\n",
       "1935    When we promote a child from grade to grade wh...\n",
       "531     ( 1 ) Legislation to authorize the President t...\n",
       "665     Beyond this great chamber , out yonder in 50 S...\n",
       "3014    We need more funding for advanced transit syst...\n",
       "                              ...                        \n",
       "1780    And when that result is achieved , our men and...\n",
       "1451    I therefore recommend that the Commission be c...\n",
       "496     For with a country as with a person , \" What i...\n",
       "1977    Fourth , to continue the priorities and alloca...\n",
       "693     In turn , I will restructure our civil service...\n",
       "799                                         ( Applause .)\n",
       "1576                           We passed welfare reform .\n",
       "373     The repatriation of Japanese military and civi...\n",
       "135     It constitutes a program of government in rela...\n",
       "1343     And I challenge the Congress to finish the job .\n",
       "514     The United States is grateful that many nation...\n",
       "799                     We shouldn ' t cap that program .\n",
       "76      Last year ' s congressional session was the lo...\n",
       "778     We should rely on Government as a partner to h...\n",
       "894     Tonight I ask the House and Senate to join me ...\n",
       "1400    Beginning next year , seniors will have new co...\n",
       "1891    To this group a tax adjustment would resul t i...\n",
       "1834    On September the 11th , 2001 , we found that p...\n",
       "1300    Executive & Cabinet Links Links to Executive ,...\n",
       "336     We do not intend to live in the midst of abund...\n",
       "1629    Their right to choose will foster competition ...\n",
       "2600    And tonight , before I close , I want to invit...\n",
       "55               They have earned our undying gratitude .\n",
       "1123    The unexpended balances will be down to 28 bil...\n",
       "2074                                        ( Applause .)\n",
       "2880    I want to thank you again and to tell you , Mr...\n",
       "141     With the passage of a full employment bill whi...\n",
       "2367    I think we ought to say to every American : Yo...\n",
       "749     I know the folks at the Crawford coffee shop c...\n",
       "513     We will ask , and we will need , the help of p...\n",
       "Name: sent, Length: 7690, dtype: object"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', #filter out stopwords\n",
    "                             lowercase=True,       #convert all to lowercase\n",
    "                             min_df=2,             #use words appearing at least twice per document\n",
    "                             max_df=0.5,           #drop words that occur in more than half of the documents\n",
    "                             use_idf=True,\n",
    "                             smooth_idf=True,\n",
    "                             norm='l2'\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying TF IDF vectorizer, let's split our data into train test set and apply the supervised models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Score : \n",
      "  [ 0.67012987  0.66211826  0.6655823   0.66167859  0.64996747]\n",
      "Training Data Avg Score: 0.661895299138\n",
      "Test Data Avg Score: 0.586611732897\n",
      "\n",
      "Random Forest Classifier\n",
      "Score : \n",
      "  [ 0.57077922  0.56985055  0.57774886  0.57059206  0.5718933 ]\n",
      "Training Data Avg Score: 0.572172799119\n",
      "Test Data Avg Score: 0.521612607314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models_default_param(X_train_tfidf, y_train, X_test_tfidf, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Score : \n",
      "  [ 0.67012987  0.66211826  0.6655823   0.66167859  0.64996747]\n",
      "Training Data Avg Score: 0.661895299138\n",
      "Test Data Avg Score: 0.586611732897\n",
      "\n",
      "Random Forest Classifier\n",
      "Score : \n",
      "  [ 0.65454545  0.63807667  0.62654522  0.64150943  0.63435264]\n",
      "Training Data Avg Score: 0.639005882926\n",
      "Test Data Avg Score: 0.581930919643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_best_param(X_train_tfidf, y_train, X_test_tfidf, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores have not improved after the TF IDF approach. Let's try get new features from unsupervised clustering models and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = MiniBatchKMeans(n_clusters=4, init='k-means++', batch_size=5000)\n",
    "\n",
    "km.fit(X_train_tfidf)\n",
    "km_train_label = km.labels_\n",
    "km_test_label = km.predict(X_test_tfidf)\n",
    "\n",
    "X_train_new_km = pd.concat([pd.DataFrame(X_train_tfidf.toarray()), pd.DataFrame(km_train_label, columns = ['Cluster'])], axis = 1)\n",
    "X_test_new_km = pd.concat([pd.DataFrame(X_test_tfidf.toarray()), pd.DataFrame(km_test_label, columns = ['Cluster'])], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Score : \n",
      "  [ 0.67142857  0.65951917  0.66363045  0.66232921  0.64996747]\n",
      "Training Data Avg Score: 0.661374974099\n",
      "Test Data Avg Score: 0.5845337968\n",
      "\n",
      "Random Forest Classifier\n",
      "Score : \n",
      "  [ 0.65584416  0.63807667  0.63695511  0.62979831  0.63044893]\n",
      "Training Data Avg Score: 0.638224634247\n",
      "Test Data Avg Score: 0.585579521963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_best_param(X_train_new_km, y_train, X_test_new_km, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the scores remained almost the same. We can take our tf-idf vector matrix one step further and perform latent semantic analysis (LSA) on the words in an attempt to gain semantic information. LSA is performed through a dimensionality reduction technique called singluar value decompositin (SVD). SVD is applied to a tf-idf vector matrix and the resulting components represent clusters of words that presumably reflect topics within the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis using Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7690, 4899)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by components: 88.2883775118\n",
      "Component 0:\n",
      "sent\n",
      "( Applause .)    0.999859\n",
      "( Applause .)    0.999859\n",
      "( Applause .)    0.999859\n",
      "( Applause .)    0.999859\n",
      "( Applause .)    0.999859\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "sent\n",
      "The people of this great country have a right to expect that the Congress and the President will work in closest cooperation with one objective - the welfare of the people of this Nation as a whole .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      0.302470\n",
      "So I ask you now in the Congress and in the country to join with me in expressing and fulfilling that faith in working for a nation , a nation that is free from want and a world that is free from hate -- a world of peace and justice , and freedom and abundance , for our time and for all time to come .                                                                                                                                                                                                                                                                                                                                                                                                                                               0.301318\n",
      "It is a time to build , to build the America within reach , an America where everybody has a chance to get ahead with hard work ; where every citizen can live in a safe community ; where families are strong , schools are good , and all our young people can go on to college ; an America where scientists find cures for diseases , from diabetes to Alzheimer ' s to AIDS ; an America where every child can stretch a hand across a keyboard and reach every book ever written , every painting ever painted , every symphony ever composed ; where government provides opportunity and citizens honor the responsibility to give something back to their communities ; an America which leads the world to new heights of peace and prosperity .    0.298109\n",
      "People who work hard still need support to get ahead in the new economy .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    0.288763\n",
      "America and the world will not be blackmailed .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              0.282968\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "sent\n",
      "Expenditures for these capital improvement programs are estimated at 30 million dollars in the fiscal year 1946 and 39 million dollars in the fiscal year 1947 .                                0.564622\n",
      "Interest payments on the public debt are estimated at 5 billion dollars in the fiscal year 1947 , an increase of 250 million dollars from the revised estimate for the current fiscal year .    0.551636\n",
      "The Rural Electrification Administration will require expenditures during the current fiscal year estimated at 156 million dollars ; in the fiscal year 1947 , at 241 million dollars .         0.551205\n",
      "We must do it this year .                                                                                                                                                                       0.537328\n",
      "They should be made this year .                                                                                                                                                                 0.537328\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "sent\n",
      "And I thank you for that .              0.781165\n",
      "I thank all of you for all of that .    0.781165\n",
      "Thank you .                             0.781165\n",
      "We thank her .                          0.781165\n",
      "Thank you .                             0.781165\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "sent\n",
      "Thank you very much .         0.582903\n",
      "Thank you .                   0.582903\n",
      "And I thank you for that .    0.582903\n",
      "I thank you for that .        0.582903\n",
      "Thank you .                   0.582903\n",
      "Name: 4, dtype: float64\n",
      "Component 5:\n",
      "sent\n",
      "The American people are working together .                                                 0.484524\n",
      "The American people know better than that .                                                0.479896\n",
      "They never seemed to know why people disliked them .                                       0.475561\n",
      "And we have to give the American people one that lives within its means .                  0.381202\n",
      "The people who most want to change this system are the people who are dependent on it .    0.379918\n",
      "Name: 5, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(2000)\n",
    "lsa_pipe = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "# Fit with training data, transform test data\n",
    "X_train_lsa = lsa_pipe.fit_transform(X_train_tfidf)\n",
    "X_test_lsa = lsa_pipe.transform(X_test_tfidf)\n",
    "\n",
    "# Examine variance captured in reduced feature space\n",
    "variance_explained = svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print('Percent variance captured by components:', total_variance*100)\n",
    "\n",
    "sent_by_component = pd.DataFrame(X_train_lsa, index = X_train)\n",
    "\n",
    "# Look at values from first 5 components\n",
    "for i in range(6):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(sent_by_component.loc[:, i].sort_values(ascending=False)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first 5 sentences of the first 6 components, we get the following semantic information:\n",
    "\n",
    "\n",
    "Component 0 - The word 'applause'. This is a corpus specific stop word that could be removed from the documents.\n",
    "\n",
    "Component 1 - Longer statements containing positive sentiments about prosperity in America, world peace.\n",
    "\n",
    "Component 2 - Statements referencing the word 'year'.\n",
    "\n",
    "Component 3  and Component 4 - Statements around 'thanks'.\n",
    "\n",
    "Component 5 - The phrase 'The American people '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7690 7690 1923 1923\n"
     ]
    }
   ],
   "source": [
    " print(len(X_train_lsa), len(y_train), len(X_test_lsa),len(y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Score : \n",
      "  [ 0.67337662  0.66601689  0.66428107  0.66297983  0.65517241]\n",
      "Training Data Avg Score: 0.664365365822\n",
      "Test Data Avg Score: 0.599602811335\n",
      "\n",
      "Random Forest Classifier\n",
      "Score : \n",
      "  [ 0.39480519  0.39246264  0.3955758   0.38646714  0.41314249]\n",
      "Training Data Avg Score: 0.396490651807\n",
      "Test Data Avg Score: 0.395213171154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_default_param(X_train_lsa , y_train, X_test_lsa, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter  {'C': 1, 'fit_intercept': True, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr_lsa_params =[ {'C': [0.01, 0.1, 1, 10],'solver':['liblinear'],'penalty':['l1', 'l2'],'fit_intercept':[True]} ]\n",
    "lr_lsa = LogisticRegression()\n",
    "gr_logr_lsa = GridSearchCV(lr_lsa,param_grid = lr_lsa_params )\n",
    "gr_logr_lsa.fit(X_train_lsa, y_train)\n",
    "print('Best Parameter ', gr_logr_lsa.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter  {'max_depth': None, 'max_features': 'auto', 'min_samples_split': 2, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rfc_lsa_params  = {\n",
    "    'n_estimators':[100,500],\n",
    "    'max_features':['auto', 'sqrt', 'log2'],\n",
    "    'max_depth':[4, 6,7, None],\n",
    "    'min_samples_split':[2, 8]\n",
    "}\n",
    "rfc_lsa = ensemble.RandomForestClassifier(random_state=10)\n",
    "rfc_grid_lsa = GridSearchCV(rfc_lsa, param_grid=rfc_lsa_params)\n",
    "rfc_grid_lsa.fit(X_train_lsa, y_train)\n",
    "\n",
    "\n",
    "print('Best Parameter ', rfc_grid_lsa.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Score : \n",
      "  [ 0.67207792  0.66796621  0.66753416  0.66232921  0.65322056]\n",
      "Training Data Avg Score: 0.664625612727\n",
      "Test Data Avg Score: 0.601166657194\n",
      "\n",
      "Random Forest Classifier\n",
      "Score : \n",
      "  [ 0.49090909  0.47823262  0.49577098  0.48275862  0.47755368]\n",
      "Training Data Avg Score: 0.485044997722\n",
      "Test Data Avg Score: 0.469577756561\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_best_param(X_train_lsa , y_train, X_test_lsa, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random forest scores are decreased a lot. But the logistic regression got 2% improvement on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Supervised and Unsupervised Learning for NLP Applications\n",
    "\n",
    "Based on the models above, it's clear, more work could be done to make a more accurate classifier. We started by removing corpus specific stop words and punctuations after a little bit of text cleaning. We explored how supervised and unsupervised methods could be applied to natural language processing applications.\n",
    "\n",
    "Here's a summary of what we learned:\n",
    "\n",
    "Unsupervised:\n",
    "\n",
    "Tf-Idf can be used to detect unique identifier words\n",
    "SVD can identify corpus specific stop words (i.e. 'applause'), as well as common phrases (i.e. 'The American people ')\n",
    "SVD clusters the tf-idf matrix into general themes and sentiments present in the documents. Sometimes these clusters are meaningful and can shed light on common sentiments.\n",
    "We didn't use similarity comparison here because it didn't make much sense for our document size. However, if the document size was larger, say the full text body for each address, running a similarity analysis could be quite informative. For example, we could compare the similarity of different presidents, or look at how similarity changes within a president's term.\n",
    "\n",
    "Supervised:\n",
    "\n",
    "The supervised methods is used for text classification purposes. Depending on the model used, additional insights could be gained by looking at feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
