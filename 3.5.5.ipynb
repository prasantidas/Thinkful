{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics.\n",
    "\n",
    "This is a regression problem — I’d probably start out with a simple linear regression model.\n",
    "then try a couple more computationally intensive models like SVR and Random Forest to see if they gave a significantly different result. \n",
    "\n",
    "\n",
    "#2. You have more features (columns) than rows in your dataset.\n",
    "\n",
    "This looks like a feature reduction problem. \n",
    "I would implement some kind of feature reduction — Maybe  Random Forest feature_importances, LASSO, or maybe eliminating features with SelectKBest, PCA, RFE.\n",
    "\n",
    "\n",
    "#3. Identify the most important characteristic predicting likelihood of being jailed before age 20.\n",
    "\n",
    "This is a problem where the goal is actually feature reduction. I’d do similar to above — use Random Forest feature_importances, RFE, and SelectKBest. I’d also be interested in knowing how some of those important features are related to each other, so I’d probably run a correlation analysis on those. \n",
    "\n",
    "\n",
    "#4. Implement a filter to “highlight” emails that might be important to the recipient\n",
    "\n",
    "This is a classifier problem. I would like to start with Naive Bayes. Maybe I could run a Random Forest classifier to pick out some of the most important features, then reduce the dimensionality of the data set by just focusing on the top few hundred predictive words. From there, it would just be a matter of seeing what does the best job — Naive Bayes, SVC, Random Forest Classifier, Decision Tree etc.\n",
    "\n",
    "\n",
    "#5. You have 1000+ features.\n",
    "\n",
    "We might want to reduce the dimensionality here. I’d see how it worked with PCA, SelectKBest, Random Forest Importances, and maybe RFE if I have a lot of processor time. \n",
    "\n",
    "\n",
    "#6. Predict whether someone who adds items to their cart on a website will purchase the items.\n",
    "\n",
    "This is a classifier problem. We could start out with Naive Bayes again, then move onto decision trees, then random forest and SVC. \n",
    "\n",
    "\n",
    "#7. Your dataset dimensions are 982400 x 500\n",
    "\n",
    "I’d want to reduce the number of features, if possible, using PCA, RFE, SelectKBest, or similar. \n",
    "\n",
    "\n",
    "#8. Identify faces in an image.\n",
    "\n",
    "This is a classifier problem, so we could use SVC, Random Forest, etc for it, but we’d probably be better served by a CNN.\n",
    "\n",
    "\n",
    "#9. Predict which of three flavors of ice cream will be most popular with boys vs girls.\n",
    "\n",
    "Another classifier problem — Decision tree, random forest, SVC, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
